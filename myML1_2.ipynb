{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myML1_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oukey/M_L/blob/master/myML1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igdr_UfSXCpE",
        "colab_type": "text"
      },
      "source": [
        "# **Занятие 2. Модели PyTorch**\n",
        "\n",
        "https://vk.com/lambda_brain\n",
        "\n",
        "Модели в машинном обучении реализуют концепцию прогноза, предсказания, в виде единой целостной сущности (класс, объект). Они же в прикладном плане служат основной для создания всех видов нейронных сетей.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVVNj5fqIa2",
        "colab_type": "text"
      },
      "source": [
        "##**Подготовка данных**\n",
        "\n",
        "Для корректного продолжения работы в текущем ноутбуке требуется выполнить ряд инициализаций из предыдущего задания. Запустите следующий код, формирующий тестовые наборы данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqXjx3EBsWLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "np.random.seed(42) \n",
        "sz = 100\n",
        "x = np.random.rand(sz, 1) \n",
        "y = 1 + 2 * x + 0.1 * np.random.randn(sz, 1) \n",
        "idx = np.arange(sz)\n",
        "np.random.shuffle(idx)\n",
        "sz80 = (int)(sz*0.8)\n",
        "train_idx = idx[: sz80]\n",
        "val_idx = idx[sz80:]\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]\n",
        "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyjZGmRVXlr0",
        "colab_type": "text"
      },
      "source": [
        "Модель PyTorch представляет собой обычный класс Python, унаследованный от класса **nn.Module**.\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Module\n",
        "\n",
        "Самых важных методов, которые потребуется переопределить в нашей модели, два:\n",
        "\n",
        "1) **стандартный конструктор** \n",
        "\n",
        "В конструкторе задаются базовые атрибуты, в нашем случае это два параметра a и b. Они определяются в конструкторе с помощью специального типа nn.Parameter.\n",
        "\n",
        "Количество атрибутов модели не ограничено.\n",
        "\n",
        "2) **forward()**, непосредственно выполняющий все нужные вычисления на основании входных данных (они задаются как параметр этого метода).\n",
        "\n",
        "Однако даже сами эти методы не потребуется вызывать напрямую. **В PyTorch \"запускается\" сама модель в целом**, и создание нужного объекта и вызов forward() происходят автоматически -- они так же скрыты под капотом.\n",
        "\n",
        "Вот как будет выглядеть наша модель для линейной регрессии:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sg4zKSt7gbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "class ManualLinearRegression(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # два наших параметра a и b \n",
        "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # формула линейной регрессии\n",
        "        return self.a + self.b * x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbMVxmZyYdtU",
        "colab_type": "text"
      },
      "source": [
        "Тип nn.Parameter очень удобен тем, что мы можем автоматизировать многие моменты, связанные с обработкой параметров. Например метод parameters() позволяет организовать итерацию по всем параметрам модели, и даже по параметрам вложенных моделей, которые могут потребоваться для настройки оптимизатора (вместо того, чтобы формировать такой список параметров вручную).\n",
        "\n",
        "Текущие значения всех параметров можно получить с помощью метода state_dict().\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ4Lu_lrY1nO",
        "colab_type": "text"
      },
      "source": [
        "**Важно.** Все наши выборки надо располагать на том же девайсе, где мы размещаем и модель. Это необходимо из соображений эффективности. Напомню, что данные основных типов PyTorch загружаются на девайс методом to().\n",
        "\n",
        "\n",
        "\n",
        "Наконец, нам ещё потребуется вызвать вручную метод модели **train()**, который на самом деле ничего не делает, а просто переводит модель в стандартный обучающий режим. Существует и ряд других режимов работы модели (например, верификация), поэтому данный флажок надо устанавливать явно.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEobzwlg-MI-",
        "colab_type": "code",
        "outputId": "990c8c99-a936-4906-f6ee-9dff97163dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# создаём модель\n",
        "model = ManualLinearRegression().to(device)\n",
        "print(model.state_dict())\n",
        "\n",
        "lr = 0.1\n",
        "n_epochs = 1000 \n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train() # режим обучения\n",
        "    \n",
        "    yhat = model(x_train_tensor) # \"запускаем\" модель с входными данными\n",
        "    \n",
        "    loss = loss_fn(yhat, y_train_tensor)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()  \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
            "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-w3zxZ8ZNrg",
        "colab_type": "text"
      },
      "source": [
        "Получим такие результаты, фактически не отличающиеся от предыдущих версий программы:\n",
        "\n",
        "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
        "\n",
        "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n",
        "\n",
        "Теперь в основном коде мы вообще избавились от всех явных упоминаний параметров a и b! Чтобы изучить поведение любых других моделей, достаточно лишь сменить название класса ManualLinearRegression, причём это можно прозрачно автоматизировать. \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOjhYe_QZYAs",
        "colab_type": "text"
      },
      "source": [
        "##**Вложенные модели**\n",
        "\n",
        "PyTorch -- очень мощный по своим возможностям фреймворк. Он создавался, когда в машинном обучении уже был накоплен немалый опыт, и поэтому воплотил многие сильные идеи, которые в других ML-фреймворках либо не реализованы, либо реализованы плохо или неудобно.\n",
        "\n",
        "Казалось бы, мы ушли почти от всей ручной работы, и теперь работаем только с абстракцией модели, где задаём конкретную формулу прогноза и пару параметров, которые в этой формуле используются. Однако для большинства прогнозов можно воспользоваться уже готовыми моделями PyTorch.\n",
        "\n",
        "В частности, за линейную регрессию отвечает модель Linear\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Linear\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEW3L_m6ViWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerLinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI_0PO2XZsQ8",
        "colab_type": "text"
      },
      "source": [
        "Вместо создания двух атрибутов линейной регрессии мы готовим в нашей модели всего один атрибут linear, который будет хранить вложенную модель Linear. В её конструкторе указываются два параметра -- количество наборов данных на входе и на выходе. И в том, и в другом случае их по одному: Linear(1, 1). А в методе forward() нашей модели мы просто \"запускаем\" вложенную модель Linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSGPGofRVyF8",
        "colab_type": "code",
        "outputId": "e99d9573-6191-43a2-82ad-fb208332e2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = LayerLinearRegression().to(device)\n",
        "print(model.state_dict())\n",
        "\n",
        "lr = 0.1\n",
        "n_epochs = 1000 \n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    yhat = model(x_train_tensor)\n",
        "    loss = loss_fn(yhat, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(model.state_dict())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n",
            "OrderedDict([('linear.weight', tensor([[1.9690]])), ('linear.bias', tensor([1.0235]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcGCqmgFZ8dY",
        "colab_type": "text"
      },
      "source": [
        "##**Последовательные модели**\n",
        "\n",
        "Пока мы использовали совсем простую модель в один шаг (слой) с помощью линейной регрессии, однако PyTorch активнее всего применяется для создания нейронных сетей, концепция которых довольно проста: есть наборы слоёв, каждый из которых это по сути функция над тензорами, и данные просто прогоняются через такую последовательность слоёв (выход одного слоя есть вход для следующего).\n",
        "\n",
        "Такая схема поддерживается в PyTorch последовательной моделью (тип nn.Sequential). В конструкторе она получает серию моделей, которые внутри автоматически связываются в последовательность вычислений через входы-выходы.\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Sequential\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJmR8XGJaTlO",
        "colab_type": "text"
      },
      "source": [
        "В нашем случае мы используем всего один слой Linear, и соответствующая последовательная модель может быть сформирована так:\n",
        "\n",
        "`model = nn.Sequential(nn.Linear(1, 1)).to(device)`\n",
        "\n",
        "Таким образом, мы вообще полностью избавились от всех пользовательских формул и типов данных, полностью сконструировав наш оптимизационный алгоритм из готовых кубиков PyTorch!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh9X8nXfu03e",
        "colab_type": "text"
      },
      "source": [
        "##**Шаг обучения**\n",
        "\n",
        "Давайте взгляем на наш код ещё раз. Мы уже обобщили использование оптимизатора, функции потерь и модели. Эта схема универсальная, она по сути никак не изменится, если мы захотим использовать другой оптимизатор, или другой лосс, или другие модели. Но тогда может быть и эту схему можно ещё более обобщить?\n",
        "\n",
        "Напрашивается вариант, когда мы эти три сущности, а также метки и признаки, подаём в некоторый универсальный алгоритм просто как настроечные параметры. Такой универсальный алгоритм называется **шаг обучения**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfRVlTi5d10E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# шаг обучения\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Формируем функцию, которая выполнит один шаг обучения\n",
        "    def train_step(x, y):\n",
        "        # Переводим модель в режим обучения\n",
        "        model.train()\n",
        "        # Вычислаем прогноз\n",
        "        yhat = model(x)\n",
        "        # Считаем лосс\n",
        "        loss = loss_fn(yhat, y)\n",
        "        # Вычисляем градиенты\n",
        "        loss.backward()\n",
        "        # Обновляем параметры и обнуляем градиенты\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Возвращаем лосс\n",
        "        return loss.item()\n",
        "    \n",
        "    # Возвращаем функцию для вызова внутри цикла обучения\n",
        "    return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLSayQsUvC5c",
        "colab_type": "text"
      },
      "source": [
        "Используем этот алгоритм для нашего конкретного случая:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw8O6D8XesBg",
        "colab_type": "code",
        "outputId": "6761b554-acb9-4f7a-d12b-f4b4d23ec37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "from torch import optim, nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "print(model.state_dict())\n",
        "\n",
        "lr = 0.1\n",
        "n_epochs = 1000 \n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# создадим функцию на основе модели, лосса и оптимизатора\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Выполним очередной шаг обучения ...\n",
        "    loss = train_step(x_train_tensor, y_train_tensor)\n",
        "    \n",
        "print(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n",
            "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrsfvqAyvOcY",
        "colab_type": "text"
      },
      "source": [
        "##**Датасеты (Datasets)**\n",
        "\n",
        "Продолжим оптимизацию и генерализацию нашей и так уже весьма универсальной программы. Сейчас мы преобразовываем массивы NumPy в тензоры PyTorch, но возможно есть более общий подход к представлению данных?\n",
        "\n",
        "В PyTorch работу с данными принято вести с помощью **датасетов (Dataset, набор данных)**, которые можно трактовать как **питоновские списки кортежей, где каждый кортеж задаёт одну точку** (признак, метку, ...).\n",
        "\n",
        "В конструкторе класса Dataset мы можем задавать самые разные формы представления входных данных (списка кортежей), от двух тензоров до CSV-файлов, и т. п.\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html?source=post_page---------------------------#torch.utils.data.Dataset\n",
        "\n",
        "Совсем не обязательно загружать в датасет сразу все данные -- ведь это могут быть обучающие выборки из десятков тысяч изображений. В таком случае их лучше загружать по прямому требованию программы. Для этого предназначен метод __get_item__(), который индексирует входной набор, позволяя обращаться к его элементам по индексам как к обычному массиву (по индексу возвращается соответствующий кортеж).\n",
        "\n",
        "Метод __len__() возвращает количество элементов во всём датасете.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Подготовим класс для наших данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7TyCECiZSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_tensor, y_tensor):\n",
        "        self.x = x_tensor\n",
        "        self.y = y_tensor\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKouQy9nJhmc",
        "colab_type": "text"
      },
      "source": [
        "Исходные данные преобразуем из формата массивов NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9gwgpNNjZI-",
        "colab_type": "code",
        "outputId": "b17c468b-e4b1-4e7b-a765-c247b9eb64e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUUtDDOJ4EW",
        "colab_type": "text"
      },
      "source": [
        "Но зачем вообще нужен дополнительный класс, если мы просто используем два тензора? \n",
        "\n",
        "Действительно, если наш датасет -- всего два тензора, то можно задействовать готовый класс PyTorch, который называется TensorDataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDz1E4cxj-EM",
        "colab_type": "code",
        "outputId": "2ed6c03c-093b-47d1-bec7-b729a0d8506c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL3kK_y8KF8K",
        "colab_type": "text"
      },
      "source": [
        "Обратите внимание, что сейчас мы не загружаем датасеты в GPU (не отправляем их на девайс), и по умолчанию они работают на обычном процессоре. Если обучающая выборка большая, то хранить её лучше в оперативной памяти компьютера, а не графической платы.\n",
        "\n",
        "Но зачем всё же мы специально создаём датасеты? Потому что вместе с ними очень удобно использовать мощные загрузчики данных.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##**Загрузчик данных (DataLoader)**\n",
        "\n",
        "Пока в нашем простеньком примере для пакетного градиентного спуска использовались все данные целиком. В реальных проектах так бывает редко -- обучающие выборки могут быть очень велики. Поэтому наш датасет надо научиться делить на более мелкие наборы, **мини-пакеты**.\n",
        "\n",
        "В PyTorch для этого имеется специальный класс DataLoader\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html?source=post_page---------------------------#torch.utils.data.DataLoader\n",
        "\n",
        "Идея его проста: мы задаём, какой набор данных использовать, какой желателен размер мини-пакета, и требуется ли данные этого мини-пакета перемешивать на каждой эпохе.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysrHYYIY5uHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YImslgAZKrXy",
        "colab_type": "text"
      },
      "source": [
        "Загрузчик работает как итератор: мы просто запрашиваем у него очередную порцию данных.\n",
        "\n",
        "Получать в цикле два очередных тензора с подвыборкой можно например так:\n",
        "\n",
        "`for x_batch, y_batch in train_loader:`\n",
        "\n",
        "Тогда новая версия нашей программы будет такая:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H05Tdwgg6Kme",
        "colab_type": "code",
        "outputId": "fcb1eec9-3f14-4564-ec01-29519500037e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        \n",
        "print(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('0.weight', tensor([[1.9684]])), ('0.bias', tensor([1.0235]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3r_ebYcK-Si",
        "colab_type": "text"
      },
      "source": [
        "Обратите внимание, что теперь мы явно посылаем наши мини-пакеты на девайс, где развёрнута модель, потому что, как говорилось выше, датасеты исходно создаются в оперативной памяти. В частности, когда используются большие фермы с множеством графических плат, можно распределять эти мини-пакеты по разным GPU.\n",
        "\n",
        "И второе изменение -- добавился внутренний цикл, в котором мы последовательно получаем и обрабатываем кусочки исходной обучающей выборки.\n",
        "\n",
        "Итак, теперь по сути мы можем полностью сфокусироваться на обучающей выборке. Мы создаём датасет и загрузчик данных для него. То же самое мы делаем и для тестовой выборки, которую мы формировали вручную.\n",
        "\n",
        "Но нельзя ли автоматизировать и этот момент?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##**Случайное разделение выборки**\n",
        "\n",
        "Именно для цели разделения выборки на обучающий и тестовый наборы в PyTorch имеется метод random_split(). Только, конечно, не надо забывать, что он исходно применяется ко всему входному массиву!\n",
        "\n",
        "random_split() получает на вход исходный датасет и список из двух чисел, первое из которых задаёт (в процентах) размер обучающей выборки, а второе число -- размер тестовой выборки.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XblNWaq4CAgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwa-_aBWLa6O",
        "colab_type": "text"
      },
      "source": [
        "##**Оценка**\n",
        "\n",
        "Заключительная часть посвящена оценке успешности нашей модели, \n",
        "что можно сделать через расчёт лосса для тестовой выборки. Для этого нам надо добавить дополнительный цикл итераций по мини-пакетам уже не обучающей, а тестовой выборки, и отправить их на тот же девайс, где и модель.\n",
        "\n",
        "Тут надо учеть два момента: во-первых, нам для этого уже не нужно считать градиенты, потому что выборка не обучающая, а тестовая (применяем упомянутый выше torch.no_grad() ко всему такому циклу), и во-вторых, модель надо явно перевести из обучающего режима в оценочный/тестовый с помощью метода eval().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_pKl5deHwy",
        "colab_type": "code",
        "outputId": "56a71906-1f0f-494d-bae0-af19e3c0e514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # обучающая выборка\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        \n",
        "    # тестовая выборка        \n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "            model.eval()\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(yhat, y_val)\n",
        "\n",
        "print(model.state_dict())\n",
        "print(loss, val_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('0.weight', tensor([[1.9102]])), ('0.bias', tensor([1.0389]))])\n",
            "0.006248112302273512 tensor(0.0081)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geXpBERlMay4",
        "colab_type": "text"
      },
      "source": [
        "##**Задание**\n",
        "\n",
        "Оформите ваш код из последнего задания первого занятия так, чтобы он представлял собой работу с моделью.\n",
        "Добавьте верификацию модели по тестовой выборке, как в последнем примере.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J32PvN-LZjbl",
        "colab_type": "text"
      },
      "source": [
        "Решение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RG9a-vYZ3dk",
        "colab_type": "code",
        "outputId": "3e7b22cf-d42c-4a30-97cf-ecea177a9ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "'''Класс модели'''\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # два параметра a и b\n",
        "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # формула\n",
        "        return self.a * x ** 2 + self.b\n",
        "\n",
        "\n",
        "'''Шаг обучения'''\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Формирует функцию, которая выполнит один шаг обучения\n",
        "    def train_step(x, y):\n",
        "        model.train()  # Переводим модель в режим обучения\n",
        "        yhat = model(x)  # Вычисление прогноза\n",
        "        loss = loss_fn(yhat, y)  # Рассчет лосс\n",
        "        loss.backward()  # вычисление градиентов\n",
        "        optimizer.step()  # Обновление параметров и обнуление градиентов\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item()\n",
        "    return train_step\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Настройка дефайса\n",
        "np.random.seed(42)  # Инициализация повторяемой последовательности рандомных чисел \n",
        "sz = 100  # Создание массива из 100 случайных ноликов/единичек\n",
        "x = np.random.rand(sz, 1)\n",
        "y = 7 * x ** 2 + 3 + 0.1 * np.random.randn(sz, 1)  # Построение функции\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "# создание модели\n",
        "model = MyModel().to(device)\n",
        "lr = 0.1\n",
        "n_epochs = 1000\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "print('Исходные значения: a = 7, b = 3')\n",
        "print('Значения до обучения: ', model.state_dict())\n",
        "\n",
        "# запуск шага обучения на основе модели, лосса и оптимизатора\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Обучающая выборка\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "    # тестовая выборка\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "            model.eval()\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(yhat, y_val)\n",
        "\n",
        "print('Значения после обучения', model.state_dict())\n",
        "print('Оценка успешности модели:')\n",
        "print('loss = ', loss)\n",
        "print('val_loss = ', val_loss)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Исходные значения: a = 7, b = 3\n",
            "Значения до обучения:  OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
            "Значения после обучения OrderedDict([('a', tensor([6.9780])), ('b', tensor([3.0178]))])\n",
            "Оценка успешности модели:\n",
            "loss =  0.009535685181617737\n",
            "val_loss =  tensor(0.0147)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pFZz4vDMKcU",
        "colab_type": "text"
      },
      "source": [
        "##**Итог**\n",
        "\n",
        "В первых двух занятиях мы изучили почти все ключевые шаги, которые делаются в практических проектах на PyTorch при разработке и анализе моделей машинного обучения!\n",
        "\n",
        "Далее мы будем рассматривать конкретные прикладные примеры и приёмы использования PyTorch с помощью нашей универсальной схемы.\n"
      ]
    }
  ]
}